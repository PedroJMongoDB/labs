{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "from pyspark.sql.types import IntegerType\r\n",
        "\r\n",
        "df = spark.read.load('abfss://wwi-02@asadatalakepjk5qt0.dfs.core.windows.net/sale-small/Year=2019/Quarter=Q4/Month=12/Day=20191231/sale-small-20191231-snappy.parquet', format='parquet')\r\n",
        "\r\n",
        "df = df.withColumn(\"Quantity\", df[\"Quantity\"].cast(IntegerType()))\r\n",
        "df = df.withColumn(\"Hour\", df[\"Hour\"].cast(IntegerType()))\r\n",
        "df = df.withColumn(\"Minute\", df[\"Minute\"].cast(IntegerType()))\r\n",
        "\r\n",
        "\r\n",
        "df.createOrReplaceTempView(\"dfViewSales\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": "8",
              "statement_id": 22,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-15T16:39:00.3004191Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-15T16:39:00.4160645Z",
              "execution_finish_time": "2022-07-15T16:39:00.9192081Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(SparkPool01, 8, 22, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "//Add required imports\n",
        "import org.apache.spark.sql.DataFrame\n",
        "import org.apache.spark.sql.SaveMode\n",
        "import com.microsoft.spark.sqlanalytics.utils.Constants\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\n",
        "\n",
        "val writeOptionsWithAADAuth:Map[String, String] = Map(Constants.SERVER -> \"SQLPool01.sql.azuresynapse.net\",\n",
        "                                            Constants.TEMP_FOLDER -> \"abfss://tempdata@asaworkspacepjk5qt0.dfs.core.windows.net/tempviernes\")\n",
        "\n",
        "val df = spark.sql(\"select * from dfViewSales\")\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "df.write.\n",
        "    mode(SaveMode.Overwrite).\n",
        "    synapsesql(tableName = \"SQLPool01.dbo.salesSparkParquet3\", \n",
        "                //For external table type value is Constants.EXTERNAL\n",
        "                tableType = Constants.INTERNAL, \n",
        "                //Optional parameter that is used to specify external table's base folder; defaults to `database_name/schema_name/table_name`\n",
        "                location = None)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": "8",
              "statement_id": 26,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-15T16:40:48.24789Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-15T16:40:48.3602338Z",
              "execution_finish_time": "2022-07-15T16:42:50.2453107Z",
              "spark_jobs": null
            },
            "text/plain": "StatementMeta(SparkPool01, 8, 26, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- TransactionId: string (nullable = true)\n |-- CustomerId: integer (nullable = true)\n |-- ProductId: short (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Price: decimal(38,18) (nullable = true)\n |-- TotalAmount: decimal(38,18) (nullable = true)\n |-- TransactionDate: integer (nullable = true)\n |-- ProfitAmount: decimal(38,18) (nullable = true)\n |-- Hour: integer (nullable = true)\n |-- Minute: integer (nullable = true)\n |-- StoreId: short (nullable = true)\n\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.SaveMode\nimport com.microsoft.spark.sqlanalytics.utils.Constants\nimport org.apache.spark.sql.SqlAnalyticsConnector._\nwriteOptionsWithAADAuth: Map[String,String] = Map(logical_server -> SQLPool01.sql.azuresynapse.net, templocation -> abfss://tempdata@asaworkspacepjk5qt0.dfs.core.windows.net/tempviernes)\ndf: org.apache.spark.sql.DataFrame = [TransactionId: string, CustomerId: int ... 9 more fields]\n"
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "scala"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}